[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "xiaoai",
    "section": "",
    "text": "This file will become your README and also the index of your documentation.",
    "crumbs": [
      "xiaoai"
    ]
  },
  {
    "objectID": "index.html#developer-guide",
    "href": "index.html#developer-guide",
    "title": "xiaoai",
    "section": "Developer Guide",
    "text": "Developer Guide\nIf you are new to using nbdev here are some useful pointers to get you started.\n\nInstall xiaoai in Development mode\n# make sure xiaoai package is installed in development mode\n$ pip install -e .\n\n# make changes under nbs/ directory\n# ...\n\n# compile to have changes apply to xiaoai\n$ nbdev_prepare",
    "crumbs": [
      "xiaoai"
    ]
  },
  {
    "objectID": "index.html#usage",
    "href": "index.html#usage",
    "title": "xiaoai",
    "section": "Usage",
    "text": "Usage\n\nInstallation\nInstall latest from the GitHub repository:\n$ pip install git+https://github.com/ForBo7/xiaoai.git\nor from conda\n$ conda install -c ForBo7 xiaoai\nor from pypi\n$ pip install xiaoai\n\n\nDocumentation\nDocumentation can be found hosted on this GitHub repository’s pages. Additionally you can find package manager specific guidelines on conda and pypi respectively.",
    "crumbs": [
      "xiaoai"
    ]
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "xiaoai",
    "section": "How to use",
    "text": "How to use\nFill me in please! Don’t forget code examples:\n\n1+1\n\n2",
    "crumbs": [
      "xiaoai"
    ]
  },
  {
    "objectID": "core.html#hugging-face-datasets",
    "href": "core.html#hugging-face-datasets",
    "title": "core",
    "section": "Hugging Face Datasets",
    "text": "Hugging Face Datasets\n\nimport logging\n\n\nlogging.disable(logging.WARNING)\n\n\nfrom datasets import load_dataset_builder\n\n\n\n\n\nname = 'fashion_mnist'\nds_builder = load_dataset_builder(name); print(ds_builder.info.description)\n\n\ndir(ds_builder.info)\n\n['_INCLUDED_INFO_IN_YAML',\n '__annotations__',\n '__class__',\n '__dataclass_fields__',\n '__dataclass_params__',\n '__delattr__',\n '__dict__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__getstate__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__le__',\n '__lt__',\n '__match_args__',\n '__module__',\n '__ne__',\n '__new__',\n '__post_init__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__setattr__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n '__weakref__',\n '_dump_info',\n '_dump_license',\n '_from_yaml_dict',\n '_to_yaml_dict',\n 'builder_name',\n 'citation',\n 'config_name',\n 'copy',\n 'dataset_name',\n 'dataset_size',\n 'description',\n 'download_checksums',\n 'download_size',\n 'features',\n 'from_dict',\n 'from_directory',\n 'from_merge',\n 'homepage',\n 'license',\n 'post_processed',\n 'post_processing_size',\n 'size_in_bytes',\n 'splits',\n 'supervised_keys',\n 'task_templates',\n 'update',\n 'version',\n 'write_to_directory']\n\n\n\nds_builder.info.splits\n\n{'train': SplitInfo(name='train', num_bytes=31304707, num_examples=60000, shard_lengths=None, dataset_name='fashion_mnist'),\n 'test': SplitInfo(name='test', num_bytes=5235160, num_examples=10000, shard_lengths=None, dataset_name='fashion_mnist')}\n\n\n\nds_builder.info.features\n\n{'image': Image(mode=None, decode=True, id=None),\n 'label': ClassLabel(names=['T - shirt / top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'], id=None)}\n\n\n\nfrom datasets import load_dataset\n\nDatasetDict({\n    train: Dataset({\n        features: ['image', 'label'],\n        num_rows: 60000\n    })\n    test: Dataset({\n        features: ['image', 'label'],\n        num_rows: 10000\n    })\n})\n\n\n\ndsd = load_dataset(name); dsd\n\n\ntrn,tst= dsd['train'],dsd['test']; trn[0]\n\n{'image': &lt;PIL.PngImagePlugin.PngImageFile image mode=L size=28x28&gt;,\n 'label': 9}\n\n\n\nx,y = ds_builder.info.features; x,y\n\n('image', 'label')\n\n\n\ntrn\n\nDataset({\n    features: ['image', 'label'],\n    num_rows: 60000\n})\n\n\n\nimg = trn[0][x]; img\n\n\n\n\n\n\n\n\n\nxb,yb = trn[:5][x],trn[:5][y]; yb\n\n[9, 0, 0, 3, 0]\n\n\n\ntrn.features\n\n{'image': Image(mode=None, decode=True, id=None),\n 'label': ClassLabel(names=['T - shirt / top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'], id=None)}\n\n\n\nfeaty = trn.features[y]; featy\n\nClassLabel(names=['T - shirt / top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'], id=None)\n\n\n\n?featy.int2str\n\nSignature: featy.int2str(values: Union[int, collections.abc.Iterable]) -&gt; Union[str, collections.abc.Iterable]\nDocstring:\nConversion `integer` =&gt; class name `string`.\n\nRegarding unknown/missing labels: passing negative integers raises `ValueError`.\n\nExample:\n\n```py\n&gt;&gt;&gt; from datasets import load_dataset\n&gt;&gt;&gt; ds = load_dataset(\"rotten_tomatoes\", split=\"train\")\n&gt;&gt;&gt; ds.features[\"label\"].int2str(0)\n'neg'\n```\nFile:      ~/miniforge3/envs/default/lib/python3.11/site-packages/datasets/features/features.py\nType:      method\n\n\n\nfeaty.int2str(yb), yb\n\n(['Ankle boot',\n  'T - shirt / top',\n  'T - shirt / top',\n  'Dress',\n  'T - shirt / top'],\n [9, 0, 0, 3, 0])\n\n\n\nimport torchvision.transforms.functional as TF\n\ntorch.Size([1, 28, 28])\n\n\n\nTF.to_tensor(img).shape\n\n\n[TF.to_tensor(o) for o in xb];\n\n\ntorch.stack([TF.to_tensor(o) for o in xb]).shape\n\n\ndef collate_fn(b):\n  return {x: torch. stack([TF.to_tensor(o[x]) for o in b]),\n          y: torch.tensor([o[y]               for o in b])}\n\n\nfrom torch.utils.data import DataLoader\n\nInit signature:\nDataLoader(\n    dataset: torch.utils.data.dataset.Dataset[+T_co],\n    batch_size: Optional[int] = 1,\n    shuffle: Optional[bool] = None,\n    sampler: Union[torch.utils.data.sampler.Sampler, Iterable, NoneType] = None,\n    batch_sampler: Union[torch.utils.data.sampler.Sampler[List], Iterable[List], NoneType] = None,\n    num_workers: int = 0,\n    collate_fn: Optional[Callable[[List[~T]], Any]] = None,\n    pin_memory: bool = False,\n    drop_last: bool = False,\n    timeout: float = 0,\n    worker_init_fn: Optional[Callable[[int], NoneType]] = None,\n    multiprocessing_context=None,\n    generator=None,\n    *,\n    prefetch_factor: Optional[int] = None,\n    persistent_workers: bool = False,\n    pin_memory_device: str = '',\n)\nDocstring:     \nData loader combines a dataset and a sampler, and provides an iterable over the given dataset.\n\nThe :class:`~torch.utils.data.DataLoader` supports both map-style and\niterable-style datasets with single- or multi-process loading, customizing\nloading order and optional automatic batching (collation) and memory pinning.\n\nSee :py:mod:`torch.utils.data` documentation page for more details.\n\nArgs:\n    dataset (Dataset): dataset from which to load the data.\n    batch_size (int, optional): how many samples per batch to load\n        (default: ``1``).\n    shuffle (bool, optional): set to ``True`` to have the data reshuffled\n        at every epoch (default: ``False``).\n    sampler (Sampler or Iterable, optional): defines the strategy to draw\n        samples from the dataset. Can be any ``Iterable`` with ``__len__``\n        implemented. If specified, :attr:`shuffle` must not be specified.\n    batch_sampler (Sampler or Iterable, optional): like :attr:`sampler`, but\n        returns a batch of indices at a time. Mutually exclusive with\n        :attr:`batch_size`, :attr:`shuffle`, :attr:`sampler`,\n        and :attr:`drop_last`.\n    num_workers (int, optional): how many subprocesses to use for data\n        loading. ``0`` means that the data will be loaded in the main process.\n        (default: ``0``)\n    collate_fn (Callable, optional): merges a list of samples to form a\n        mini-batch of Tensor(s).  Used when using batched loading from a\n        map-style dataset.\n    pin_memory (bool, optional): If ``True``, the data loader will copy Tensors\n        into device/CUDA pinned memory before returning them.  If your data elements\n        are a custom type, or your :attr:`collate_fn` returns a batch that is a custom type,\n        see the example below.\n    drop_last (bool, optional): set to ``True`` to drop the last incomplete batch,\n        if the dataset size is not divisible by the batch size. If ``False`` and\n        the size of dataset is not divisible by the batch size, then the last batch\n        will be smaller. (default: ``False``)\n    timeout (numeric, optional): if positive, the timeout value for collecting a batch\n        from workers. Should always be non-negative. (default: ``0``)\n    worker_init_fn (Callable, optional): If not ``None``, this will be called on each\n        worker subprocess with the worker id (an int in ``[0, num_workers - 1]``) as\n        input, after seeding and before data loading. (default: ``None``)\n    multiprocessing_context (str or multiprocessing.context.BaseContext, optional): If\n        ``None``, the default `multiprocessing context`_ of your operating system will\n        be used. (default: ``None``)\n    generator (torch.Generator, optional): If not ``None``, this RNG will be used\n        by RandomSampler to generate random indexes and multiprocessing to generate\n        ``base_seed`` for workers. (default: ``None``)\n    prefetch_factor (int, optional, keyword-only arg): Number of batches loaded\n        in advance by each worker. ``2`` means there will be a total of\n        2 * num_workers batches prefetched across all workers. (default value depends\n        on the set value for num_workers. If value of num_workers=0 default is ``None``.\n        Otherwise, if value of ``num_workers &gt; 0`` default is ``2``).\n    persistent_workers (bool, optional): If ``True``, the data loader will not shut down\n        the worker processes after a dataset has been consumed once. This allows to\n        maintain the workers `Dataset` instances alive. (default: ``False``)\n    pin_memory_device (str, optional): the device to :attr:`pin_memory` to if ``pin_memory`` is\n        ``True``.\n\n\n.. warning:: If the ``spawn`` start method is used, :attr:`worker_init_fn`\n             cannot be an unpicklable object, e.g., a lambda function. See\n             :ref:`multiprocessing-best-practices` on more details related\n             to multiprocessing in PyTorch.\n\n.. warning:: ``len(dataloader)`` heuristic is based on the length of the sampler used.\n             When :attr:`dataset` is an :class:`~torch.utils.data.IterableDataset`,\n             it instead returns an estimate based on ``len(dataset) / batch_size``, with proper\n             rounding depending on :attr:`drop_last`, regardless of multi-process loading\n             configurations. This represents the best guess PyTorch can make because PyTorch\n             trusts user :attr:`dataset` code in correctly handling multi-process\n             loading to avoid duplicate data.\n\n             However, if sharding results in multiple workers having incomplete last batches,\n             this estimate can still be inaccurate, because (1) an otherwise complete batch can\n             be broken into multiple ones and (2) more than one batch worth of samples can be\n             dropped when :attr:`drop_last` is set. Unfortunately, PyTorch can not detect such\n             cases in general.\n\n             See `Dataset Types`_ for more details on these two types of datasets and how\n             :class:`~torch.utils.data.IterableDataset` interacts with\n             `Multi-process data loading`_.\n\n.. warning:: See :ref:`reproducibility`, and :ref:`dataloader-workers-random-seed`, and\n             :ref:`data-loading-randomness` notes for random seed related questions.\n\n.. _multiprocessing context:\n    https://docs.python.org/3/library/multiprocessing.html#contexts-and-start-methods\nFile:           ~/miniforge3/envs/default/lib/python3.11/site-packages/torch/utils/data/dataloader.py\nType:           type\nSubclasses:     \n\n\n\n?DataLoader\n\n\ndl = DataLoader(trn, collate_fn=collate_fn, batch_size=16)\nb = next(iter(dl))\nb[x].shape, b[y]\n\n(torch.Size([16, 1, 28, 28]),\n tensor([9, 0, 0, 3, 0, 2, 7, 2, 5, 5, 0, 9, 5, 5, 7, 9]))\n\n\nInstead of ahving to convert all images to tensors when the dataloader is created, we can do so when only an individual batch is created.\n\ndef transforms(b):\n  b[x] = [TF.to_tensor(o) for o in b[x]]\n  return b\n\nIn this case, a collation function wil not be needed. This is because either PyTorch know show to handle HF’s with_transform method, or PyTorch will supply its own [appropriate] collate function.\n\ntdsd = trn.with_transform(transforms)\ndl = DataLoader(tdsd, batch_size=16)\nb = next(iter(dl))\nb[x].shape, b[y]\n\n(torch.Size([16, 1, 28, 28]),\n tensor([9, 0, 0, 3, 0, 2, 7, 2, 5, 5, 0, 9, 5, 5, 7, 9]))\n\n\nTo prevent having to include a return statement in our transforms funciton, we can create an additional function to handle this for us.\n\nTF.to_tensor(img).shape, torch.flatten(TF.to_tensor(img)).shape\n\n(torch.Size([1, 28, 28]), torch.Size([784]))\n\n\n\ndef _transformi(b): b[x] = [torch.flatten(TF.to_tensor(o)) for o in b[x]]\n\n\nsource\n\ninplace\n\n inplace (f)\n\n\ntransformi = inplace(_transformi)\n\n\nr = trn.with_transform(transformi)[0]\nr[x].shape, r[y]\n\n(torch.Size([784]), 9)\n\n\nIn fact, this is simply what a decorator is.\n\n@inplace\ndef transformi(b): b[x] = [torch.flatten(TF.to_tensor(o)) for o in b[x]]\n\n\ntdsd = trn.with_transform(transformi)\nr = tdsd[0]\nr[x].shape, r[y]\n\n(torch.Size([784]), 9)\n\n\n\n\nItemgetters\n\nimport operator\n\nCall signature: operator(obj, /, *args, **kwargs)\nType:           module\nString form:    &lt;module 'operator' from '/Users/salmannaqvi/miniforge3/envs/default/lib/python3.11/operator.py'&gt;\nFile:           ~/miniforge3/envs/default/lib/python3.11/operator.py\nDocstring:     \nOperator interface.\n\nThis module exports a set of functions implemented in C corresponding\nto the intrinsic operators of Python.  For example, operator.add(x, y)\nis equivalent to the expression x+y.  The function names are those\nused for special methods; variants without leading and trailing\n'__' are also provided for convenience.\nCall docstring: Same as obj(*args, **kwargs).\n\n\n\n?operator\n\n\n?operator\n\n\n?itemgetter\n\n\nd = dict(a=1,b=2,c=3)\nig = itemgetter('a','c'); ig(d)\n\n(1, 3)\n\n\n\nclass D:\n  def __getitem__(self,k): return 1 if k=='a' else 2 if k=='b' else 3\n\n\nd = D(); d['a']\n\n1\n\n\n\nig(d)\n\n(1, 3)\n\n\n\n\nCollation\n\nfrom torch.utils.data import default_collate\n\nSignature: default_collate(batch)\nDocstring:\nTake in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\n\nThe exact output type can be a :class:`torch.Tensor`, a `Sequence` of :class:`torch.Tensor`, a\nCollection of :class:`torch.Tensor`, or left unchanged, depending on the input type.\nThis is used as the default function for collation when\n`batch_size` or `batch_sampler` is defined in :class:`~torch.utils.data.DataLoader`.\n\nHere is the general input type (based on the type of the element within the batch) to output type mapping:\n\n    * :class:`torch.Tensor` -&gt; :class:`torch.Tensor` (with an added outer dimension batch size)\n    * NumPy Arrays -&gt; :class:`torch.Tensor`\n    * `float` -&gt; :class:`torch.Tensor`\n    * `int` -&gt; :class:`torch.Tensor`\n    * `str` -&gt; `str` (unchanged)\n    * `bytes` -&gt; `bytes` (unchanged)\n    * `Mapping[K, V_i]` -&gt; `Mapping[K, default_collate([V_1, V_2, ...])]`\n    * `NamedTuple[V1_i, V2_i, ...]` -&gt; `NamedTuple[default_collate([V1_1, V1_2, ...]),\n      default_collate([V2_1, V2_2, ...]), ...]`\n    * `Sequence[V1_i, V2_i, ...]` -&gt; `Sequence[default_collate([V1_1, V1_2, ...]),\n      default_collate([V2_1, V2_2, ...]), ...]`\n\nArgs:\n    batch: a single batch to be collated\n\nExamples:\n    &gt;&gt;&gt; # xdoctest: +SKIP\n    &gt;&gt;&gt; # Example with a batch of `int`s:\n    &gt;&gt;&gt; default_collate([0, 1, 2, 3])\n    tensor([0, 1, 2, 3])\n    &gt;&gt;&gt; # Example with a batch of `str`s:\n    &gt;&gt;&gt; default_collate(['a', 'b', 'c'])\n    ['a', 'b', 'c']\n    &gt;&gt;&gt; # Example with `Map` inside the batch:\n    &gt;&gt;&gt; default_collate([{'A': 0, 'B': 1}, {'A': 100, 'B': 100}])\n    {'A': tensor([  0, 100]), 'B': tensor([  1, 100])}\n    &gt;&gt;&gt; # Example with `NamedTuple` inside the batch:\n    &gt;&gt;&gt; Point = namedtuple('Point', ['x', 'y'])\n    &gt;&gt;&gt; default_collate([Point(0, 0), Point(1, 1)])\n    Point(x=tensor([0, 1]), y=tensor([0, 1]))\n    &gt;&gt;&gt; # Example with `Tuple` inside the batch:\n    &gt;&gt;&gt; default_collate([(0, 1), (2, 3)])\n    [tensor([0, 2]), tensor([1, 3])]\n    &gt;&gt;&gt; # Example with `List` inside the batch:\n    &gt;&gt;&gt; default_collate([[0, 1], [2, 3]])\n    [tensor([0, 2]), tensor([1, 3])]\n    &gt;&gt;&gt; # Two options to extend `default_collate` to handle specific type\n    &gt;&gt;&gt; # Option 1: Write custom collate function and invoke `default_collate`\n    &gt;&gt;&gt; def custom_collate(batch):\n    ...     elem = batch[0]\n    ...     if isinstance(elem, CustomType):  # Some custom condition\n    ...         return ...\n    ...     else:  # Fall back to `default_collate`\n    ...         return default_collate(batch)\n    &gt;&gt;&gt; # Option 2: In-place modify `default_collate_fn_map`\n    &gt;&gt;&gt; def collate_customtype_fn(batch, *, collate_fn_map=None):\n    ...     return ...\n    &gt;&gt;&gt; default_collate_fn_map.update(CustomType, collate_customtype_fn)\n    &gt;&gt;&gt; default_collate(batch)  # Handle `CustomType` automatically\nFile:      ~/miniforge3/envs/default/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\nType:      function\n\n\n\n?default_collate\n\n\nbatch = dict(a=[1],b=[2]), dict(a=[3],b=[4])\ndefault_collate(batch), type(batch)\n\n({'a': [tensor([1, 3])], 'b': [tensor([2, 4])]}, tuple)\n\n\n\nig = itemgetter(*trn.features); ig\n\noperator.itemgetter('image', 'label')\n\n\n\ndefault_collate([b])\n\n{'image': tensor([[[[[0., 0., 0.,  ..., 0., 0., 0.],\n            [0., 0., 0.,  ..., 0., 0., 0.],\n            [0., 0., 0.,  ..., 0., 0., 0.],\n            ...,\n            [0., 0., 0.,  ..., 0., 0., 0.],\n            [0., 0., 0.,  ..., 0., 0., 0.],\n            [0., 0., 0.,  ..., 0., 0., 0.]]],\n \n \n          [[[0., 0., 0.,  ..., 0., 0., 0.],\n            [0., 0., 0.,  ..., 0., 0., 0.],\n            [0., 0., 0.,  ..., 0., 0., 0.],\n            ...,\n            [0., 0., 0.,  ..., 0., 0., 0.],\n            [0., 0., 0.,  ..., 0., 0., 0.],\n            [0., 0., 0.,  ..., 0., 0., 0.]]],\n \n \n          [[[0., 0., 0.,  ..., 0., 0., 0.],\n            [0., 0., 0.,  ..., 0., 0., 0.],\n            [0., 0., 0.,  ..., 0., 0., 0.],\n            ...,\n            [0., 0., 0.,  ..., 0., 0., 0.],\n            [0., 0., 0.,  ..., 0., 0., 0.],\n            [0., 0., 0.,  ..., 0., 0., 0.]]],\n \n \n          ...,\n \n \n          [[[0., 0., 0.,  ..., 0., 0., 0.],\n            [0., 0., 0.,  ..., 0., 0., 0.],\n            [0., 0., 0.,  ..., 0., 0., 0.],\n            ...,\n            [0., 0., 0.,  ..., 0., 0., 0.],\n            [0., 0., 0.,  ..., 0., 0., 0.],\n            [0., 0., 0.,  ..., 0., 0., 0.]]],\n \n \n          [[[0., 0., 0.,  ..., 0., 0., 0.],\n            [0., 0., 0.,  ..., 0., 0., 0.],\n            [0., 0., 0.,  ..., 0., 0., 0.],\n            ...,\n            [0., 0., 0.,  ..., 0., 0., 0.],\n            [0., 0., 0.,  ..., 0., 0., 0.],\n            [0., 0., 0.,  ..., 0., 0., 0.]]],\n \n \n          [[[0., 0., 0.,  ..., 0., 0., 0.],\n            [0., 0., 0.,  ..., 0., 0., 0.],\n            [0., 0., 0.,  ..., 0., 0., 0.],\n            ...,\n            [0., 0., 0.,  ..., 0., 0., 0.],\n            [0., 0., 0.,  ..., 0., 0., 0.],\n            [0., 0., 0.,  ..., 0., 0., 0.]]]]]),\n 'label': tensor([[9, 0, 0, 3, 0, 2, 7, 2, 5, 5, 0, 9, 5, 5, 7, 9]])}\n\n\n\nig(default_collate([b]))\n\n(tensor([[[[[0., 0., 0.,  ..., 0., 0., 0.],\n            [0., 0., 0.,  ..., 0., 0., 0.],\n            [0., 0., 0.,  ..., 0., 0., 0.],\n            ...,\n            [0., 0., 0.,  ..., 0., 0., 0.],\n            [0., 0., 0.,  ..., 0., 0., 0.],\n            [0., 0., 0.,  ..., 0., 0., 0.]]],\n \n \n          [[[0., 0., 0.,  ..., 0., 0., 0.],\n            [0., 0., 0.,  ..., 0., 0., 0.],\n            [0., 0., 0.,  ..., 0., 0., 0.],\n            ...,\n            [0., 0., 0.,  ..., 0., 0., 0.],\n            [0., 0., 0.,  ..., 0., 0., 0.],\n            [0., 0., 0.,  ..., 0., 0., 0.]]],\n \n \n          [[[0., 0., 0.,  ..., 0., 0., 0.],\n            [0., 0., 0.,  ..., 0., 0., 0.],\n            [0., 0., 0.,  ..., 0., 0., 0.],\n            ...,\n            [0., 0., 0.,  ..., 0., 0., 0.],\n            [0., 0., 0.,  ..., 0., 0., 0.],\n            [0., 0., 0.,  ..., 0., 0., 0.]]],\n \n \n          ...,\n \n \n          [[[0., 0., 0.,  ..., 0., 0., 0.],\n            [0., 0., 0.,  ..., 0., 0., 0.],\n            [0., 0., 0.,  ..., 0., 0., 0.],\n            ...,\n            [0., 0., 0.,  ..., 0., 0., 0.],\n            [0., 0., 0.,  ..., 0., 0., 0.],\n            [0., 0., 0.,  ..., 0., 0., 0.]]],\n \n \n          [[[0., 0., 0.,  ..., 0., 0., 0.],\n            [0., 0., 0.,  ..., 0., 0., 0.],\n            [0., 0., 0.,  ..., 0., 0., 0.],\n            ...,\n            [0., 0., 0.,  ..., 0., 0., 0.],\n            [0., 0., 0.,  ..., 0., 0., 0.],\n            [0., 0., 0.,  ..., 0., 0., 0.]]],\n \n \n          [[[0., 0., 0.,  ..., 0., 0., 0.],\n            [0., 0., 0.,  ..., 0., 0., 0.],\n            [0., 0., 0.,  ..., 0., 0., 0.],\n            ...,\n            [0., 0., 0.,  ..., 0., 0., 0.],\n            [0., 0., 0.,  ..., 0., 0., 0.],\n            [0., 0., 0.,  ..., 0., 0., 0.]]]]]),\n tensor([[9, 0, 0, 3, 0, 2, 7, 2, 5, 5, 0, 9, 5, 5, 7, 9]]))\n\n\n\nsource\n\n\ncollate_dict\n\n collate_dict (ds)\n\nThe function above collates a dictionary. it converts a dictionary into a tuple, containing the xs and ys. Handy when working with both Hugging Face and PyTorch.\n\ndl = DataLoader(tdsd, batch_size=4, collate_fn=collate_dict(tdsd))\nxb,yb = next(iter(dl))\nxb.shape, yb\n\n(torch.Size([4, 784]), tensor([9, 0, 0, 3]))\n\n\n\nxb,yb\n\n(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]]),\n tensor([9, 0, 0, 3]))",
    "crumbs": [
      "core"
    ]
  },
  {
    "objectID": "core.html#plotting-images",
    "href": "core.html#plotting-images",
    "title": "core",
    "section": "Plotting Images",
    "text": "Plotting Images\n\nb = next(iter(dl)); b\n\n(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]]),\n tensor([9, 0, 0, 3]))\n\n\n\nxb = b[0]\n# img = xb[0]; img.shape, img[0].shape\n\n\nimport matplotlib as mpl\n\n\nmpl.rcParams['image.cmap'] = 'gray'\n\n\nplt.imshow(img)\n\n\nimg = TF.to_tensor(img)\n\n\nimg.shape, img.permute(1,2,0).shape\n\n(torch.Size([1, 28, 28]), torch.Size([28, 28, 1]))\n\n\n\n?fc.delegates\n\n\n?fc.hasattrs\n\nSignature: fc.hasattrs(o, attrs)\nDocstring: Test whether `o` contains all `attrs`\nFile:      ~/miniforge3/envs/default/lib/python3.11/site-packages/fastcore/basics.py\nType:      function\n\n\n/opt/hostedtoolcache/Python/3.10.15/x64/lib/python3.10/site-packages/fastcore/docscrape.py:230: UserWarning: Unknown section Other Parameters\n  else: warn(msg)\n/opt/hostedtoolcache/Python/3.10.15/x64/lib/python3.10/site-packages/fastcore/docscrape.py:230: UserWarning: Unknown section See Also\n  else: warn(msg)\n\nsource\n\nshow_image\n\n show_image (im, ax=None, figsize=None, title=None, noframe=True,\n             cmap=None, norm=None, aspect=None, interpolation=None,\n             alpha=None, vmin=None, vmax=None, origin=None, extent=None,\n             interpolation_stage=None, filternorm=True, filterrad=4.0,\n             resample=None, url=None, data=None)\n\nShow a PIL or PyTorch image on ax.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nim\n\n\n\n\n\nax\nNoneType\nNone\n\n\n\nfigsize\nNoneType\nNone\n\n\n\ntitle\nNoneType\nNone\n\n\n\nnoframe\nbool\nTrue\n\n\n\ncmap\nNoneType\nNone\nThe Colormap instance or registered colormap name used to map scalar datato colors.This parameter is ignored if X is RGB(A).\n\n\nnorm\nNoneType\nNone\nThe normalization method used to scale scalar data to the [0, 1] rangebefore mapping to colors using cmap. By default, a linear scaling isused, mapping the lowest value to 0 and the highest to 1.If given, this can be one of the following:- An instance of .Normalize or one of its subclasses (see :ref:colormapnorms).- A scale name, i.e. one of “linear”, “log”, “symlog”, “logit”, etc. For a list of available scales, call matplotlib.scale.get_scale_names(). In that case, a suitable .Normalize subclass is dynamically generated and instantiated.This parameter is ignored if X is RGB(A).\n\n\naspect\nNoneType\nNone\nThe aspect ratio of the Axes. This parameter is particularlyrelevant for images since it determines whether data pixels aresquare.This parameter is a shortcut for explicitly calling.Axes.set_aspect. See there for further details.- ‘equal’: Ensures an aspect ratio of 1. Pixels will be square (unless pixel sizes are explicitly made non-square in data coordinates using extent).- ‘auto’: The Axes is kept fixed and the aspect is adjusted so that the data fit in the Axes. In general, this will result in non-square pixels.Normally, None (the default) means to use :rc:image.aspect. However, ifthe image uses a transform that does not contain the axes data transform,then None means to not modify the axes aspect at all (in that case, directlycall .Axes.set_aspect if desired).\n\n\ninterpolation\nNoneType\nNone\nThe interpolation method used.Supported values are ‘none’, ‘antialiased’, ‘nearest’, ‘bilinear’,‘bicubic’, ‘spline16’, ‘spline36’, ‘hanning’, ‘hamming’, ‘hermite’,‘kaiser’, ‘quadric’, ‘catrom’, ‘gaussian’, ‘bessel’, ‘mitchell’,‘sinc’, ‘lanczos’, ‘blackman’.The data X is resampled to the pixel size of the image on thefigure canvas, using the interpolation method to either up- ordownsample the data.If interpolation is ‘none’, then for the ps, pdf, and svgbackends no down- or upsampling occurs, and the image data ispassed to the backend as a native image. Note that different ps,pdf, and svg viewers may display these raw pixels differently. Onother backends, ‘none’ is the same as ‘nearest’.If interpolation is the default ‘antialiased’, then ‘nearest’interpolation is used if the image is upsampled by more than afactor of three (i.e. the number of display pixels is at leastthree times the size of the data array). If the upsampling rate issmaller than 3, or the image is downsampled, then ‘hanning’interpolation is used to act as an anti-aliasing filter, unless theimage happens to be upsampled by exactly a factor of two or one.See:doc:/gallery/images_contours_and_fields/interpolation_methodsfor an overview of the supported interpolation methods, and:doc:/gallery/images_contours_and_fields/image_antialiasing fora discussion of image antialiasing.Some interpolation methods require an additional radius parameter,which can be set by filterrad. Additionally, the antigrain imageresize filter is controlled by the parameter filternorm.\n\n\nalpha\nNoneType\nNone\nThe alpha blending value, between 0 (transparent) and 1 (opaque).If alpha is an array, the alpha blending values are applied pixelby pixel, and alpha must have the same shape as X.\n\n\nvmin\nNoneType\nNone\n\n\n\nvmax\nNoneType\nNone\n\n\n\norigin\nNoneType\nNone\nPlace the [0, 0] index of the array in the upper left or lowerleft corner of the Axes. The convention (the default) ‘upper’ istypically used for matrices and images.Note that the vertical axis points upward for ‘lower’but downward for ‘upper’.See the :ref:imshow_extent tutorial forexamples and a more detailed description.\n\n\nextent\nNoneType\nNone\nThe bounding box in data coordinates that the image will fill.These values may be unitful and match the units of the Axes.The image is stretched individually along x and y to fill the box.The default extent is determined by the following conditions.Pixels have unit size in data coordinates. Their centers are oninteger coordinates, and their center coordinates range from 0 tocolumns-1 horizontally and from 0 to rows-1 vertically.Note that the direction of the vertical axis and thus the defaultvalues for top and bottom depend on origin:- For origin == 'upper' the default is (-0.5, numcols-0.5, numrows-0.5, -0.5).- For origin == 'lower' the default is (-0.5, numcols-0.5, -0.5, numrows-0.5).See the :ref:imshow_extent tutorial forexamples and a more detailed description.\n\n\ninterpolation_stage\nNoneType\nNone\nIf ‘data’, interpolationis carried out on the data provided by the user. If ‘rgba’, theinterpolation is carried out after the colormapping has beenapplied (visual interpolation).\n\n\nfilternorm\nbool\nTrue\nA parameter for the antigrain image resize filter (see theantigrain documentation). If filternorm is set, the filternormalizes integer values and corrects the rounding errors. Itdoesn’t do anything with the source floating point values, itcorrects only integers according to the rule of 1.0 which meansthat any sum of pixel weights must be equal to 1.0. So, thefilter function must produce a graph of the proper shape.\n\n\nfilterrad\nfloat\n4.0\nThe filter radius for filters that have a radius parameter, i.e.when interpolation is one of: ‘sinc’, ‘lanczos’ or ‘blackman’.\n\n\nresample\nNoneType\nNone\nWhen True, use a full resampling method. When False, onlyresample when the output image is larger than the input image.\n\n\nurl\nNoneType\nNone\nSet the url of the created .AxesImage. See .Artist.set_url.\n\n\ndata\nNoneType\nNone\n\n\n\n\n\nimg.permute(1,2,0).shape, img.permute(1,2,0)[...,0].shape\n\n(torch.Size([28, 28, 1]), torch.Size([28, 28]))\n\n\n\nshow_image(img, figsize=(2,2));\n\n\n\n\n\n\n\n\n\n?show_image\n\nSignature:\nshow_image(\n    im,\n    ax=None,\n    figsize=None,\n    title=None,\n    noframe=True,\n    *,\n    cmap=None,\n    norm=None,\n    aspect=None,\n    interpolation=None,\n    alpha=None,\n    vmin=None,\n    vmax=None,\n    origin=None,\n    extent=None,\n    interpolation_stage=None,\n    filternorm=True,\n    filterrad=4.0,\n    resample=None,\n    url=None,\n    data=None,\n)\nDocstring: Show a PIL or PyTorch image on `ax`.\nFile:      /var/folders/fy/vg316qk1001227svr6d4d8l40000gn/T/ipykernel_13157/1415261445.py\nType:      function\n\n\n\nfig,axs = plt.subplots(1,2)\nshow_image(img,axs[0])\nshow_image(trn[x][1], axs[1]);\n\n\n\n\n\n\n\n\n\nsource\n\n\nsubplots\n\n subplots (nrows:int=1, ncols:int=1, figsize:tuple=None, imsize:int=3,\n           suptitle:str=None,\n           sharex:\"bool|Literal['none','all','row','col']\"=False,\n           sharey:\"bool|Literal['none','all','row','col']\"=False,\n           squeeze:bool=True, width_ratios:Sequence[float]|None=None,\n           height_ratios:Sequence[float]|None=None,\n           subplot_kw:dict[str,Any]|None=None,\n           gridspec_kw:dict[str,Any]|None=None, **kwargs)\n\nA figure and set of subplots to display images of imsize inches\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnrows\nint\n1\nNumber of rows in returned axes grid\n\n\nncols\nint\n1\nNumber of cols in returned axes grid\n\n\nfigsize\ntuple\nNone\nw,h in inches of the returned figure\n\n\nimsize\nint\n3\nSize (in inches) of images that will be displayed in the returned figure\n\n\nsuptitle\nstr\nNone\nTitle to be set in returned figure\n\n\nsharex\nbool | Literal[‘none’, ‘all’, ‘row’, ‘col’]\nFalse\n\n\n\nsharey\nbool | Literal[‘none’, ‘all’, ‘row’, ‘col’]\nFalse\n\n\n\nsqueeze\nbool\nTrue\n\n\n\nwidth_ratios\nSequence[float] | None\nNone\n\n\n\nheight_ratios\nSequence[float] | None\nNone\n\n\n\nsubplot_kw\ndict[str, Any] | None\nNone\n\n\n\ngridspec_kw\ndict[str, Any] | None\nNone\n\n\n\nkwargs\n\n\n\n\n\n\n\nfig,axs = subplots(3,3, imsize=1)\nimgs = trn[x][:8]\nfor ax,img in zip(axs.flat, imgs): show_image(img, ax)\n\n\n\n\n\n\n\n\n\nsource\n\n\nget_grid\n\n get_grid (n:int, nrows:int=None, ncols:int=None, title:str=None,\n           weight:str='bold', size:int=14, figsize:tuple=None,\n           imsize:int=3, suptitle:str=None,\n           sharex:\"bool|Literal['none','all','row','col']\"=False,\n           sharey:\"bool|Literal['none','all','row','col']\"=False,\n           squeeze:bool=True, width_ratios:Sequence[float]|None=None,\n           height_ratios:Sequence[float]|None=None,\n           subplot_kw:dict[str,Any]|None=None,\n           gridspec_kw:dict[str,Any]|None=None)\n\nReturn a grid of n axes, rows by cols\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn\nint\n\nNumber of axes\n\n\nnrows\nint\nNone\nNumber of rows, defaulting to int(math.sqrt(n))\n\n\nncols\nint\nNone\nNumber of cols, defaulting to ceil(n/rows)\n\n\ntitle\nstr\nNone\nIf passed, title set to the figure\n\n\nweight\nstr\nbold\nTitle font weight\n\n\nsize\nint\n14\nTitle font size\n\n\nfigsize\ntuple\nNone\nw,h in inches of the returned figure\n\n\nimsize\nint\n3\nSize (in inches) of images that will be displayed in the returned figure\n\n\nsuptitle\nstr\nNone\nTitle to be set in returned figure\n\n\nsharex\nbool | Literal[‘none’, ‘all’, ‘row’, ‘col’]\nFalse\n\n\n\nsharey\nbool | Literal[‘none’, ‘all’, ‘row’, ‘col’]\nFalse\n\n\n\nsqueeze\nbool\nTrue\n\n\n\nwidth_ratios\nSequence[float] | None\nNone\n\n\n\nheight_ratios\nSequence[float] | None\nNone\n\n\n\nsubplot_kw\ndict[str, Any] | None\nNone\n\n\n\ngridspec_kw\ndict[str, Any] | None\nNone\n\n\n\n\n\nfig,axs = get_grid(8, nrows=3, imsize=1)\nfor ax,img in zip(axs.flat,imgs): show_image(img, ax)\n\n\n\n\n\n\n\n\n\n?zip_longest\n\n\nsource\n\n\nshow_images\n\n show_images (ims:list, nrows:int|None=None, ncols:int|None=None,\n              titles:list|None=None, figsize:tuple=None, imsize:int=3,\n              suptitle:str=None,\n              sharex:\"bool|Literal['none','all','row','col']\"=False,\n              sharey:\"bool|Literal['none','all','row','col']\"=False,\n              squeeze:bool=True, width_ratios:Sequence[float]|None=None,\n              height_ratios:Sequence[float]|None=None,\n              subplot_kw:dict[str,Any]|None=None,\n              gridspec_kw:dict[str,Any]|None=None)\n\nShow all images ims as subplots with rows using titles\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nims\nlist\n\nImages to show\n\n\nnrows\nint | None\nNone\nNumber of rows in grid\n\n\nncols\nint | None\nNone\nNumber of columsn in grid (auto-calculated if None)\n\n\ntitles\nlist | None\nNone\nOptional list of titles for each image\n\n\nfigsize\ntuple\nNone\nw,h in inches of the returned figure\n\n\nimsize\nint\n3\nSize (in inches) of images that will be displayed in the returned figure\n\n\nsuptitle\nstr\nNone\nTitle to be set in returned figure\n\n\nsharex\nbool | Literal[‘none’, ‘all’, ‘row’, ‘col’]\nFalse\n\n\n\nsharey\nbool | Literal[‘none’, ‘all’, ‘row’, ‘col’]\nFalse\n\n\n\nsqueeze\nbool\nTrue\n\n\n\nwidth_ratios\nSequence[float] | None\nNone\n\n\n\nheight_ratios\nSequence[float] | None\nNone\n\n\n\nsubplot_kw\ndict[str, Any] | None\nNone\n\n\n\ngridspec_kw\ndict[str, Any] | None\nNone\n\n\n\n\n\nyb = trn[y][:8]\nlbls = trn[y][:8]\n\n\nnames = 'Top Trouser Pullover Dress Coat Sandal Shirt Sneaker Bag Boot'.split()\ntitles = itemgetter(*lbls)(names)\n' '.join(titles)\n\n'Boot Top Top Dress Top Pullover Sneaker Pullover'\n\n\n\nshow_images(imgs, imsize=1.7, titles=titles)",
    "crumbs": [
      "core"
    ]
  }
]